{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "XOR_Class.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNA3KSiRAj/6HwMMWgR7DAo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/2020-nlp-c/nlp-deeplearning/blob/master/tkyang/XOR_Class.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GSiJKhqN5o6w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "ef52e449-d9cf-47d4-d264-32abe0594dc5"
      },
      "source": [
        "import numpy as np\n",
        "class XOR() :\n",
        "    def _init_weights(self, h):\n",
        "        W1 = np.random.randn(2,h)\n",
        "        B1 = np.random.randn(h,1)\n",
        "        W2 = np.random.randn(h,1)\n",
        "        B2 = np.random.randn(1,1)\n",
        "\n",
        "        return W1, B1, W2, B2\n",
        "\n",
        "    def _affine(self, W, X, B):\n",
        "        return np.dot(W.T, X) + B\n",
        "\n",
        "    def _sigmoid(self, o):\n",
        "        return 1 / (1 + np.exp(-o))\n",
        "\n",
        "    def _eval_loss(self, X, Y, Y_hat):\n",
        "        loss = -1 / X.shape[1] * np.sum(Y[0] * np.log(Y_hat) + (1-Y[0]) * np.log(1-Y_hat))\n",
        "        return loss\n",
        "\n",
        "    def _gradients(self, X, Y, W2, H, Y_hat):\n",
        "        # BackPropagate: Hidden Layer\n",
        "        dW2 = np.dot(H, (Y_hat-Y).T)\n",
        "        dB2 = 1. / Y.shape[1] * np.sum(Y_hat-Y, axis=1, keepdims=True)\n",
        "        dH  = np.dot(W2, Y_hat-Y)\n",
        "\n",
        "        # BackPropagate: Input Layer\n",
        "        dZ1 = dH * H * (1-H)\n",
        "        dW1 = np.dot(X, dZ1.T)\n",
        "        dB1 = 1. / Y.shape[1] * np.sum(dZ1, axis=1, keepdims=True)\n",
        "        return dW1, dB1, dW2, dB2\n",
        "\n",
        "    def optimize(self, X, Y, h, learning_rate, epoch):\n",
        "        W1, B1, W2, B2 = self._init_weights(h)\n",
        "\n",
        "        for epoch in range(epoch):\n",
        "            Z1 = self._affine(W1, X, B1)\n",
        "            H = self._sigmoid(Z1)\n",
        "            Z2 = self._affine(W2, H, B2)\n",
        "            Y_hat = self._sigmoid(Z2)\n",
        "\n",
        "            loss = self._eval_loss(X, Y, Y_hat)\n",
        "            dW1, dB1, dW2, dB2 = self._gradients(X, Y, W2, H, Y_hat)\n",
        "            W2 += -learning_rate * dW2\n",
        "            B2 += -learning_rate * dB2\n",
        "            W1 += -learning_rate * dW1\n",
        "            B1 += -learning_rate * dB1\n",
        "\n",
        "        Predict = []\n",
        "        for i in range(len(Y_hat[0])):\n",
        "            if Y_hat[0][i] > 0.5:\n",
        "                Predict.append(1)\n",
        "            else:\n",
        "                Predict.append(0)\n",
        "        print('Loss : ', loss)\n",
        "        print('Predict : ', Predict)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\" :\n",
        "    X = np.array([[0, 0, 1, 1], [0, 1, 0, 1]])\n",
        "    Y = np.array([[0, 1, 1, 0]])\n",
        "    xor = XOR()\n",
        "    xor.optimize(X, Y, h=3, learning_rate = 0.1, epoch = 100000)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss :  0.00021807717671367365\n",
            "Predict :  [0, 1, 1, 0]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss :  [8.74155823e-05] \n",
      " Y_hat :  [[6.92409126e-05 9.99896653e-01 9.99897080e-01 7.41386794e-05]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "class XOR():\n",
    "    def __init__(self, X, Y, i_node=2, h_node=2, learning_rate=0.1, epoch=100):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.i_node = i_node\n",
    "        self.h_node = h_node\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epoch = epoch\n",
    "        self.loss = 100\n",
    "        self.n = len(Y[0])\n",
    "\n",
    "    def __init_gradient(self):\n",
    "        self.w1 = np.random.random(size=(self.i_node, self.h_node))\n",
    "        self.w2 = np.random.random(size=(self.h_node, 1))\n",
    "        self.b1 = np.random.random(size=(self.h_node, 1))\n",
    "        self.b2 = np.random.random(size=(1, 1))\n",
    "\n",
    "    def cal_h(self):\n",
    "        sum_h = np.dot(self.w1.T, X) + self.b1\n",
    "        self.h = 1 / (1 + np.exp(-sum_h))\n",
    "\n",
    "    def cal_y_hat(self):\n",
    "        sum_y_hat = np.dot(self.w2.T, self.h) + self.b2\n",
    "        self.y_hat = 1 / (1 + np.exp(-sum_y_hat))\n",
    "\n",
    "    def cal_loss(self):\n",
    "        self.loss = -1 / self.n * sum(np.dot(Y, np.log(self.y_hat).T) + np.dot((1 - Y), np.log(1 - self.y_hat).T))\n",
    "\n",
    "    def cal_gradient(self):\n",
    "        # Gradient 계산\n",
    "        dif = self.y_hat - Y\n",
    "        c_w1 = np.dot(X, (np.dot(self.w2, dif) * self.h * (1 - self.h)).T)\n",
    "        c_w2 = np.dot(self.h, dif.T)\n",
    "        c_b1 = np.dot(self.w2, dif) * self.h * (1-self.h)\n",
    "\n",
    "        # update weight\n",
    "        self.w1 = self.w1 - self.learning_rate * c_w1\n",
    "        self.w2 = self.w2 - self.learning_rate*c_w2\n",
    "        self.b1 = self.b1 - self.learning_rate * c_b1\n",
    "        self.b2 = self.b2 - self.learning_rate * dif\n",
    "\n",
    "    def run(self):\n",
    "        self.__init_gradient()\n",
    "\n",
    "        for i in range(self.epoch):\n",
    "            before_loss = self.loss\n",
    "\n",
    "            self.cal_h()\n",
    "            self.cal_y_hat()\n",
    "            self.cal_loss()\n",
    "            self.cal_gradient()\n",
    "\n",
    "            if self.loss == before_loss or self.loss == 0:\n",
    "                print(\"loss : \", self.loss, \"\\n\", \"Y_hat : \", self.y_hat)\n",
    "                break\n",
    "\n",
    "        print(\"loss : \", self.loss, \"\\n\", \"Y_hat : \", self.y_hat)\n",
    "\n",
    "\n",
    "X = np.array([[0, 0, 1, 1], [0, 1, 0, 1]])\n",
    "Y = np.array([[0, 1, 1, 0]])\n",
    "\n",
    "xor = XOR(X, Y, learning_rate=0.1, epoch=100000)\n",
    "xor.run()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

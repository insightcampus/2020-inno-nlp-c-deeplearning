{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "14 Word2Vec Practice.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN7RrI8gE0OsKDfWeSe7HcA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/2020-nlp-c/nlp-deeplearning/blob/master/jisang/14_Word2Vec_Practice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YXq_-24eqhZ9",
        "colab_type": "text"
      },
      "source": [
        "# **Word2Vec 실습**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zMXVIvfEuZ9b",
        "colab_type": "text"
      },
      "source": [
        "## **1. Word2Vec 구현**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LWGy78rWkIRb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "doc = 'you will never know until you try'"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dxwyd783kY_6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "outputId": "705ed78c-30f1-4ce4-9e96-64edee6d7f95"
      },
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_Z2VLJ6qmfY",
        "colab_type": "text"
      },
      "source": [
        "### **1-1. 데이터 전처리**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IMpJR-dkkdXL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "49e3886e-3c64-42e3-d7ce-747f9539ab73"
      },
      "source": [
        "from nltk import pos_tag\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "wl = WordNetLemmatizer()\n",
        "\n",
        "# 문장 전처리\n",
        "def tokenize(x):\n",
        "    return x.split()\n",
        "words = tokenize(doc)\n",
        "\n",
        "tmp_docs = []\n",
        "# Lemmatize\n",
        "for word in words:\n",
        "    tmp_docs.append(wl.lemmatize(word.lower(), pos = 'v' or 'n'))\n",
        "# Pos Tagging\n",
        "pos_docs = pos_tag(tmp_docs)\n",
        "\n",
        "# 불용어 처리(stopWord)\n",
        "stopPos = ['CC']\n",
        "stopWord = [',']\n",
        "\n",
        "docs_tokens = []\n",
        "tokens = []\n",
        "\n",
        "for pos_doc in pos_docs:\n",
        "    # 불용 품사 지정\n",
        "    if pos_doc[1] not in stopPos:\n",
        "        # 불용어 지정\n",
        "        if pos_doc[0] not in stopWord:\n",
        "            # 문서 사용 단어\n",
        "            docs_tokens.append(pos_doc[0])\n",
        "\n",
        "# 전체 사용 단어\n",
        "tokens = list(set(docs_tokens))\n",
        "\n",
        "docs_tokens, tokens"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['you', 'will', 'never', 'know', 'until', 'you', 'try'],\n",
              " ['until', 'try', 'know', 'never', 'will', 'you'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aE4bWhBnk0ZZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "outputId": "02e8623f-12bc-4bb5-87ac-33ab8684db57"
      },
      "source": [
        "from numpy import argmax\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# 문자열 라벨링\n",
        "label_enc = LabelEncoder()\n",
        "label_docs = label_enc.fit_transform(docs_tokens)\n",
        "# 바이너리 인코딩\n",
        "onehot_enc = OneHotEncoder(sparse=False)\n",
        "docs_label = label_docs.reshape(len(label_docs), 1) # n:1 matrix로 변환\n",
        "onehot_docs = onehot_enc.fit_transform(docs_label)\n",
        "    \n",
        "label_enc.inverse_transform([5]), label_enc.transform(['you'])\n",
        "\n",
        "onehot_docs"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., 0., 0., 1.],\n",
              "       [0., 0., 0., 0., 1., 0.],\n",
              "       [0., 1., 0., 0., 0., 0.],\n",
              "       [1., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 1., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 1.],\n",
              "       [0., 0., 1., 0., 0., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bCy483eqpuA",
        "colab_type": "text"
      },
      "source": [
        "### **1-2. Window 생성**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "feRboHnnrIOi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        },
        "outputId": "8c7ea6c3-89d3-49d3-ab8c-38b46dbbe2ef"
      },
      "source": [
        "window_size = 1\n",
        "x = []\n",
        "y = []\n",
        "\n",
        "for i in range(len(onehot_docs)):\n",
        "    tmp = []\n",
        "    for j in range(i-window_size, i+window_size + 1):\n",
        "        if j < 0:\n",
        "            pass\n",
        "        elif j > len(onehot_docs):\n",
        "            pass\n",
        "        elif 0 <= j < len(onehot_docs):\n",
        "            if i != j:\n",
        "                tmp.append(onehot_docs[j])\n",
        "    x.append(tmp)\n",
        "    y.append(onehot_docs[i])\n",
        "\n",
        "x, y"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([[array([0., 0., 0., 0., 1., 0.])],\n",
              "  [array([0., 0., 0., 0., 0., 1.]), array([0., 1., 0., 0., 0., 0.])],\n",
              "  [array([0., 0., 0., 0., 1., 0.]), array([1., 0., 0., 0., 0., 0.])],\n",
              "  [array([0., 1., 0., 0., 0., 0.]), array([0., 0., 0., 1., 0., 0.])],\n",
              "  [array([1., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 1.])],\n",
              "  [array([0., 0., 0., 1., 0., 0.]), array([0., 0., 1., 0., 0., 0.])],\n",
              "  [array([0., 0., 0., 0., 0., 1.])]],\n",
              " [array([0., 0., 0., 0., 0., 1.]),\n",
              "  array([0., 0., 0., 0., 1., 0.]),\n",
              "  array([0., 1., 0., 0., 0., 0.]),\n",
              "  array([1., 0., 0., 0., 0., 0.]),\n",
              "  array([0., 0., 0., 1., 0., 0.]),\n",
              "  array([0., 0., 0., 0., 0., 1.]),\n",
              "  array([0., 0., 1., 0., 0., 0.])])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y635qKuPqtsZ",
        "colab_type": "text"
      },
      "source": [
        "### **1-3. Feed Foward**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ifZKuWZ8IdO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 650
        },
        "outputId": "3d5e85d3-1b08-4c25-962a-79c59a7be380"
      },
      "source": [
        "k = 4\n",
        "\n",
        "# X to Hidden Layer Weight\n",
        "x2h = np.random.rand(len(tokens), k)\n",
        "# Hidden Layer to Y Weight\n",
        "h2y = np.random.rand(k, len(tokens))\n",
        "\n",
        "hidden = []\n",
        "for words in x:\n",
        "    tmp = []\n",
        "    for word in words:\n",
        "        tmp.append(np.dot(x2h.T, word.T))\n",
        "    hidden.append(tmp)\n",
        "\n",
        "def softmax(a) :\n",
        "    exp_a = np.exp(a)\n",
        "    sum_exp_a = np.sum(exp_a)\n",
        "    y = exp_a / sum_exp_a\n",
        "    \n",
        "    return y\n",
        "\n",
        "y_predict = []\n",
        "for words in hidden:\n",
        "    tmp = []\n",
        "    for word in words:\n",
        "        tmp.append(softmax(np.dot(h2y.T, word)))\n",
        "    y_predict.append(tmp)\n",
        "\n",
        "hidden, y_predict"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([[array([0.72970348, 0.32447501, 0.44634518, 0.88234414])],\n",
              "  [array([0.30761346, 0.83321515, 0.71273927, 0.90391157]),\n",
              "   array([0.53302861, 0.85609795, 0.02766275, 0.24314283])],\n",
              "  [array([0.72970348, 0.32447501, 0.44634518, 0.88234414]),\n",
              "   array([0.82640854, 0.97962196, 0.9062349 , 0.31222941])],\n",
              "  [array([0.53302861, 0.85609795, 0.02766275, 0.24314283]),\n",
              "   array([0.9604504 , 0.92983219, 0.46588585, 0.93577407])],\n",
              "  [array([0.82640854, 0.97962196, 0.9062349 , 0.31222941]),\n",
              "   array([0.30761346, 0.83321515, 0.71273927, 0.90391157])],\n",
              "  [array([0.9604504 , 0.92983219, 0.46588585, 0.93577407]),\n",
              "   array([0.80062386, 0.45316813, 0.87202302, 0.32035027])],\n",
              "  [array([0.30761346, 0.83321515, 0.71273927, 0.90391157])]],\n",
              " [[array([0.16237503, 0.19945652, 0.174693  , 0.19113796, 0.17723433,\n",
              "          0.09510316])],\n",
              "  [array([0.16297551, 0.17665972, 0.13061324, 0.23516236, 0.21227977,\n",
              "          0.08230941]),\n",
              "   array([0.12623359, 0.15729578, 0.18079359, 0.21209712, 0.20860824,\n",
              "          0.11497168])],\n",
              "  [array([0.16237503, 0.19945652, 0.174693  , 0.19113796, 0.17723433,\n",
              "          0.09510316]),\n",
              "   array([0.13274843, 0.21680811, 0.21278177, 0.17865008, 0.17368353,\n",
              "          0.08532809])],\n",
              "  [array([0.12623359, 0.15729578, 0.18079359, 0.21209712, 0.20860824,\n",
              "          0.11497168]),\n",
              "   array([0.13349754, 0.18465657, 0.17674852, 0.22286042, 0.20568461,\n",
              "          0.07655235])],\n",
              "  [array([0.13274843, 0.21680811, 0.21278177, 0.17865008, 0.17368353,\n",
              "          0.08532809]),\n",
              "   array([0.16297551, 0.17665972, 0.13061324, 0.23516236, 0.21227977,\n",
              "          0.08230941])],\n",
              "  [array([0.13349754, 0.18465657, 0.17674852, 0.22286042, 0.20568461,\n",
              "          0.07655235]),\n",
              "   array([0.14943047, 0.23413076, 0.22165023, 0.15051294, 0.14703849,\n",
              "          0.0972371 ])],\n",
              "  [array([0.16297551, 0.17665972, 0.13061324, 0.23516236, 0.21227977,\n",
              "          0.08230941])]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rOaVTBYkq2Rx",
        "colab_type": "text"
      },
      "source": [
        "### **1-4. Loss 계산**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aud9KmXpiIBz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b3809167-ecfc-4f37-9847-c704d5924870"
      },
      "source": [
        "loss = []\n",
        "for predict in y_predict:\n",
        "    tmp = []\n",
        "    for j in range(len(predict)):\n",
        "        tmp.append(np.log(predict[j]) * y[i])\n",
        "    loss.append(np.sum(tmp))\n",
        "loss = -np.sum(loss)\n",
        "loss"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "21.084478866546398"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yk_q2SZQq5JJ",
        "colab_type": "text"
      },
      "source": [
        "### **1-5. Back Propagation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AhBQIZVflD73",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        },
        "outputId": "6b2bb635-e61e-463c-8a85-8083fee0d864"
      },
      "source": [
        "alpha = 0.01\n",
        "diff = []\n",
        "h_layer = []\n",
        "x_layer = []\n",
        "\n",
        "for i in range(len(y)):\n",
        "    for j in range(len(y_predict[i])):\n",
        "        diff.append(y_predict[i][j] - y[i])\n",
        "        h_layer.append(hidden[i][j])\n",
        "        x_layer.append(x[i][j])\n",
        "\n",
        "h2y = h2y - alpha*np.dot(np.matrix(h_layer).T, np.matrix(diff))\n",
        "x2h = x2h - np.dot(np.matrix(x_layer).T, np.dot(h2y, np.matrix(diff).T).T)\n",
        "\n",
        "np.array(h2y), np.array(x2h"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[0.80169339, 0.76780406, 0.07675191, 0.2731178 , 0.55032861,\n",
              "         0.82780191],\n",
              "        [0.07021935, 0.34295988, 0.31749778, 0.30611318, 0.82239916,\n",
              "         0.95052085],\n",
              "        [0.78141985, 0.94735418, 0.3850283 , 0.97586688, 0.64185778,\n",
              "         0.7127623 ],\n",
              "        [0.64621189, 0.55805259, 0.16680369, 0.44529688, 0.15970663,\n",
              "         0.66444265]]),\n",
              " array([[-0.4633492 , -1.00943364,  2.33849919,  0.60433932],\n",
              "        [ 0.83898332, -0.08174132, -0.03381773, -0.02849969],\n",
              "        [ 1.42975708,  2.01444736,  0.21468681,  1.4644484 ],\n",
              "        [ 2.13768412,  0.49779953,  0.44330413,  1.80504363],\n",
              "        [ 1.9283531 ,  1.59863847,  1.12513142,  1.62231672],\n",
              "        [-3.57553957, -0.02662526, -0.84815106, -1.96829643]]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdSoHXc01VOT",
        "colab_type": "text"
      },
      "source": [
        "### **1-6. 예측 결과값 출력**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZGkWL2P0YtZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "outputId": "43a00ad7-5d74-41b1-cd4a-66e6a86e0843"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "index_name = []\n",
        "for i in range(len(tokens)):\n",
        "    index_name.append(label_enc.inverse_transform([i])[0])\n",
        "\n",
        "pd.DataFrame(x2h, index_name)"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>know</th>\n",
              "      <td>-0.463349</td>\n",
              "      <td>-1.009434</td>\n",
              "      <td>2.338499</td>\n",
              "      <td>0.604339</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>never</th>\n",
              "      <td>0.838983</td>\n",
              "      <td>-0.081741</td>\n",
              "      <td>-0.033818</td>\n",
              "      <td>-0.028500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>try</th>\n",
              "      <td>1.429757</td>\n",
              "      <td>2.014447</td>\n",
              "      <td>0.214687</td>\n",
              "      <td>1.464448</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>until</th>\n",
              "      <td>2.137684</td>\n",
              "      <td>0.497800</td>\n",
              "      <td>0.443304</td>\n",
              "      <td>1.805044</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>will</th>\n",
              "      <td>1.928353</td>\n",
              "      <td>1.598638</td>\n",
              "      <td>1.125131</td>\n",
              "      <td>1.622317</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>you</th>\n",
              "      <td>-3.575540</td>\n",
              "      <td>-0.026625</td>\n",
              "      <td>-0.848151</td>\n",
              "      <td>-1.968296</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              0         1         2         3\n",
              "know  -0.463349 -1.009434  2.338499  0.604339\n",
              "never  0.838983 -0.081741 -0.033818 -0.028500\n",
              "try    1.429757  2.014447  0.214687  1.464448\n",
              "until  2.137684  0.497800  0.443304  1.805044\n",
              "will   1.928353  1.598638  1.125131  1.622317\n",
              "you   -3.575540 -0.026625 -0.848151 -1.968296"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJJrirQGuTbV",
        "colab_type": "text"
      },
      "source": [
        "## **2. Word2Vec 클래스화**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p2BOMahBuv14",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "6a9cdd6f-b371-4181-c8d2-f311f3827a16"
      },
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Piea1pkDuq-2",
        "colab_type": "text"
      },
      "source": [
        "### **2-1. Word2Vec 클래스**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hok5rOysuTMU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from numpy import argmax\n",
        "from nltk import pos_tag\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "class Word2Vec():\n",
        "    def __init__(self):\n",
        "        self.stopPos = ['CC']\n",
        "        self.stopWord = [',']\n",
        "        self.docs_tokens = []\n",
        "        self.tokens = []\n",
        "        self.index_name = []\n",
        "        self.onehot_docs = []\n",
        "        self.x = []\n",
        "        self.y = []\n",
        "        self.x2h = []\n",
        "        self.h2y = []\n",
        "        self.hidden = []\n",
        "        self.y_predict = []\n",
        "        self.loss = []\n",
        "\n",
        "    # 문장 전처리\n",
        "    def wrod_preprocessing(self, doc):\n",
        "        wl = WordNetLemmatizer()\n",
        "        # 띄어쓰기 기준으로 토큰화\n",
        "        def tokenize(x):\n",
        "            return x.split()\n",
        "        words = tokenize(doc)\n",
        "        # Lemmatize\n",
        "        tmp_docs = []\n",
        "        for word in words:\n",
        "            tmp_docs.append(wl.lemmatize(word.lower(), pos = 'v' or 'n'))\n",
        "        # Pos Tagging\n",
        "        pos_docs = pos_tag(tmp_docs)\n",
        "        # 불용어 처리(stopWord)\n",
        "        for pos_doc in pos_docs:\n",
        "            # 불용 품사 지정\n",
        "            if pos_doc[1] not in self.stopPos:\n",
        "                # 불용어 지정\n",
        "                if pos_doc[0] not in self.stopWord:\n",
        "                    # 문서 사용 단어\n",
        "                    self.docs_tokens.append(pos_doc[0])\n",
        "        # 전체 사용 단어\n",
        "        self.tokens = list(set(self.docs_tokens))\n",
        "        \n",
        "        return self.docs_tokens, self.tokens\n",
        "\n",
        "    # 라벨링\n",
        "    def labeling_word(self):\n",
        "        # 문자열 라벨링\n",
        "        label_enc = LabelEncoder()\n",
        "        label_docs = label_enc.fit_transform(self.docs_tokens)\n",
        "        # 바이너리 인코딩\n",
        "        onehot_enc = OneHotEncoder(sparse=False)\n",
        "        docs_label = label_docs.reshape(len(label_docs), 1) # n:1 matrix로 변환\n",
        "        self.onehot_docs = onehot_enc.fit_transform(docs_label)\n",
        "        # 문자 라벨 순서 저장\n",
        "        for i in range(len(self.tokens)):\n",
        "            self.index_name.append(label_enc.inverse_transform([i])[0])\n",
        "\n",
        "        return self.onehot_docs, self.index_name\n",
        "\n",
        "    # window 생성\n",
        "    def make_window(self, window_size):\n",
        "        for i in range(len(self.onehot_docs)):\n",
        "            tmp = []\n",
        "            for j in range(i-window_size, i+window_size + 1):\n",
        "                if j < 0:\n",
        "                    pass\n",
        "                elif j > len(self.onehot_docs):\n",
        "                    pass\n",
        "                elif 0 <= j < len(self.onehot_docs):\n",
        "                    if i != j:\n",
        "                        tmp.append(self.onehot_docs[j])\n",
        "            self.x.append(tmp)\n",
        "            self.y.append(self.onehot_docs[i])\n",
        "\n",
        "        return self.x, self.y\n",
        "\n",
        "    # Weight 생성\n",
        "    def make_weight(self, k):\n",
        "        # X to Hidden Layer Weight\n",
        "        self.x2h = np.random.rand(len(self.tokens), k)\n",
        "        # Hidden Layer to Y Weight\n",
        "        self.h2y = np.random.rand(k, len(self.tokens))\n",
        "\n",
        "        return self.x2h, self.h2y\n",
        "\n",
        "    # Feed Foward\n",
        "    def predict_Y(self):\n",
        "        # X to Hidden Layer 과정\n",
        "        tmp_hidden = []\n",
        "        for words in self.x:\n",
        "            tmp = []\n",
        "            for word in words:\n",
        "                tmp.append(np.dot(self.x2h.T, word.T))\n",
        "            tmp_hidden.append(tmp)\n",
        "        self.hidden = tmp_hidden\n",
        "\n",
        "        # SoftMax 함수\n",
        "        def softmax(a) :\n",
        "            return np.exp(a) / np.sum(np.exp(a))\n",
        "\n",
        "        # Hidden Layer to Y\n",
        "        tmp_predict = []\n",
        "        for words in self.hidden:\n",
        "            tmp = []\n",
        "            for word in words:\n",
        "                tmp.append(softmax(np.dot(self.h2y.T, word.T)))\n",
        "            tmp_predict.append(tmp)\n",
        "        self.y_predict = tmp_predict\n",
        "\n",
        "        # Loss 값\n",
        "        loss_tmp = []\n",
        "        for predict in self.y_predict:\n",
        "            tmp = []\n",
        "            for j in range(len(predict)):\n",
        "                tmp.append(np.log(predict[j]) * self.y[i])\n",
        "            loss_tmp.append(np.sum(tmp))\n",
        "        self.loss = loss_tmp\n",
        "        self.loss = -np.sum(self.loss)\n",
        "        \n",
        "        return self.hidden, self.y_predict, self.loss\n",
        "\n",
        "    # Back Propagation 구현\n",
        "    def back_propagation(self, alpha):\n",
        "        diff = []\n",
        "        h_layer = []\n",
        "        x_layer = []\n",
        "        for i in range(len(self.y)):\n",
        "            for j in range(len(self.y_predict[i])):\n",
        "                diff.append(self.y_predict[i][j] - self.y[i])\n",
        "                h_layer.append(self.hidden[i][j])\n",
        "                x_layer.append(self.x[i][j])\n",
        "\n",
        "        # Weight 갱신\n",
        "        self.h2y = self.h2y - alpha * np.dot(np.matrix(h_layer).T, np.matrix(diff))\n",
        "        self.x2h = self.x2h - alpha * np.dot(np.matrix(x_layer).T, np.dot(self.h2y, np.matrix(diff).T).T)\n",
        "        self.h2y = np.array(h2y) # array로 재설정 안 할 경우, 무한히 matrix가 커지는 현상 발생\n",
        "        self.x2h = np.array(x2h)\n",
        "\n",
        "        return self.h2y, self.x2h\n",
        "\n",
        "    # 자동 실행\n",
        "    def run(self, doc, window_size, k, alpha, epochs):\n",
        "        self.wrod_preprocessing(doc)\n",
        "        self.labeling_word()\n",
        "        self.make_window(window_size)\n",
        "        self.make_weight(k)\n",
        "        L1 = 0\n",
        "        count_epoch = 0\n",
        "        loss_history = []\n",
        "        count_history = []\n",
        "        for i in range(epochs):\n",
        "            self.predict_Y()\n",
        "            self.back_propagation(alpha)\n",
        "            L1 = self.loss.copy()\n",
        "            count_epoch += 1\n",
        "            count_history.append(count_epoch)\n",
        "            loss_history.append(self.loss)\n",
        "        print(\"반복횟수 : {}\".format(count_epoch))\n",
        "        print(\"Loss : {}\".format(self.loss))\n",
        "        print(\"예측 결과 :\")\n",
        "        print(pd.DataFrame(self.x2h, index_name))\n",
        "        plt.plot(count_history, loss_history)\n",
        "        plt.title(\"Loss History\")\n",
        "        plt.show()"
      ],
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fcLepSxOunYQ",
        "colab_type": "text"
      },
      "source": [
        "### **2-2. 클래스 결과 확인**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X8wbYbylynan",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "w2v = Word2Vec()"
      ],
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m2AhybJ0unDv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "doc = 'you will never know until you try'"
      ],
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Q4YsUCIwsKN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 457
        },
        "outputId": "9b6d5cc8-a86a-4959-b6c0-25eb42393c5e"
      },
      "source": [
        "w2v.run(doc, 1, 4, 0.01, 10000) # 결과가 좋지 못한것으로 보아 Loss를 계산하는 과정에 문제가 있는 것 같습니다."
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "반복횟수 : 10000\n",
            "Loss : 21.084478866546398\n",
            "예측 결과 :\n",
            "              0         1         2         3\n",
            "know   0.826409  0.979622  0.906235  0.312229\n",
            "never  0.533029  0.856098  0.027663  0.243143\n",
            "try    0.800624  0.453168  0.872023  0.320350\n",
            "until  0.960450  0.929832  0.465886  0.935774\n",
            "will   0.729703  0.324475  0.446345  0.882344\n",
            "you    0.307613  0.833215  0.712739  0.903912\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}
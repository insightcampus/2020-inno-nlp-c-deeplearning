{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "14 Word2Vec Practice.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNX3AZFrBR0Vjnzul3ObWeb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/2020-nlp-c/nlp-deeplearning/blob/master/jisang/14_Word2Vec_Practice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YXq_-24eqhZ9",
        "colab_type": "text"
      },
      "source": [
        "# **Word2Vec 실습**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LWGy78rWkIRb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "doc = 'you will never know until you try'"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dxwyd783kY_6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "outputId": "942dfdb0-27e2-4089-83a5-bae1146c3448"
      },
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_Z2VLJ6qmfY",
        "colab_type": "text"
      },
      "source": [
        "## **1. 데이터 전처리**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IMpJR-dkkdXL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "7c2f7717-63c4-4bfb-cb30-dce2928fc694"
      },
      "source": [
        "from nltk import pos_tag\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "wl = WordNetLemmatizer()\n",
        "\n",
        "# 문장 전처리\n",
        "def tokenize(x):\n",
        "    return x.split()\n",
        "words = tokenize(doc)\n",
        "\n",
        "tmp_docs = []\n",
        "# Lemmatize\n",
        "for word in words:\n",
        "    tmp_docs.append(wl.lemmatize(word.lower(), pos = 'v' or 'n'))\n",
        "# Pos Tagging\n",
        "pos_docs = pos_tag(tmp_docs)\n",
        "\n",
        "# 불용어 처리(stopWord)\n",
        "stopPos = ['CC']\n",
        "stopWord = [',']\n",
        "\n",
        "docs_tokens = []\n",
        "tokens = []\n",
        "\n",
        "for pos_doc in pos_docs:\n",
        "    # 불용 품사 지정\n",
        "    if pos_doc[1] not in stopPos:\n",
        "        # 불용어 지정\n",
        "        if pos_doc[0] not in stopWord:\n",
        "            # 문서 사용 단어\n",
        "            docs_tokens.append(pos_doc[0])\n",
        "\n",
        "# 전체 사용 단어\n",
        "tokens = list(set(docs_tokens))\n",
        "\n",
        "docs_tokens, tokens"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['you', 'will', 'never', 'know', 'until', 'you', 'try'],\n",
              " ['will', 'know', 'try', 'you', 'until', 'never'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aE4bWhBnk0ZZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "outputId": "48b05d3e-bd39-46aa-ed09-6f41ddfe8941"
      },
      "source": [
        "from numpy import argmax\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# 문자열 라벨링\n",
        "label_enc = LabelEncoder()\n",
        "label_docs = label_enc.fit_transform(docs_tokens)\n",
        "# 바이너리 인코딩\n",
        "onehot_enc = OneHotEncoder(sparse=False)\n",
        "docs_label = label_docs.reshape(len(label_docs), 1) # n:1 matrix로 변환\n",
        "onehot_docs = onehot_enc.fit_transform(docs_label)\n",
        "    \n",
        "label_enc.inverse_transform([5]), label_enc.transform(['you'])\n",
        "\n",
        "onehot_docs"
      ],
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., 0., 0., 1.],\n",
              "       [0., 0., 0., 0., 1., 0.],\n",
              "       [0., 1., 0., 0., 0., 0.],\n",
              "       [1., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 1., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 1.],\n",
              "       [0., 0., 1., 0., 0., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 121
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bCy483eqpuA",
        "colab_type": "text"
      },
      "source": [
        "## **2. Window 생성**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "feRboHnnrIOi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        },
        "outputId": "15a6d971-7ef1-4a90-c083-a42d9ae8f6e7"
      },
      "source": [
        "window_size = 1\n",
        "x = []\n",
        "y = []\n",
        "\n",
        "for i in range(len(onehot_docs)):\n",
        "    tmp = []\n",
        "    for j in range(i-window_size, i+window_size + 1):\n",
        "        if j < 0:\n",
        "            pass\n",
        "        elif j > len(onehot_docs):\n",
        "            pass\n",
        "        elif 0 <= j < len(onehot_docs):\n",
        "            if i != j:\n",
        "                tmp.append(onehot_docs[j])\n",
        "    x.append(tmp)\n",
        "    y.append(onehot_docs[i])\n",
        "\n",
        "x, y"
      ],
      "execution_count": 172,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([[array([0., 0., 0., 0., 1., 0.])],\n",
              "  [array([0., 0., 0., 0., 0., 1.]), array([0., 1., 0., 0., 0., 0.])],\n",
              "  [array([0., 0., 0., 0., 1., 0.]), array([1., 0., 0., 0., 0., 0.])],\n",
              "  [array([0., 1., 0., 0., 0., 0.]), array([0., 0., 0., 1., 0., 0.])],\n",
              "  [array([1., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 1.])],\n",
              "  [array([0., 0., 0., 1., 0., 0.]), array([0., 0., 1., 0., 0., 0.])],\n",
              "  [array([0., 0., 0., 0., 0., 1.])]],\n",
              " [array([0., 0., 0., 0., 0., 1.]),\n",
              "  array([0., 0., 0., 0., 1., 0.]),\n",
              "  array([0., 1., 0., 0., 0., 0.]),\n",
              "  array([1., 0., 0., 0., 0., 0.]),\n",
              "  array([0., 0., 0., 1., 0., 0.]),\n",
              "  array([0., 0., 0., 0., 0., 1.]),\n",
              "  array([0., 0., 1., 0., 0., 0.])])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 172
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y635qKuPqtsZ",
        "colab_type": "text"
      },
      "source": [
        "## **3. Feed Foward**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ifZKuWZ8IdO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 650
        },
        "outputId": "0db93675-26a3-4247-dd09-f8d0a3662433"
      },
      "source": [
        "k = 4\n",
        "\n",
        "# X to Hidden Layer Weight\n",
        "x2h = np.random.rand(len(tokens), k)\n",
        "# Hidden Layer to Y Weight\n",
        "h2y = np.random.rand(k, len(tokens))\n",
        "\n",
        "hidden = []\n",
        "for words in x:\n",
        "    tmp = []\n",
        "    for word in words:\n",
        "        tmp.append(np.dot(x2h.T, word.T))\n",
        "    hidden.append(tmp)\n",
        "\n",
        "def softmax(a) :\n",
        "    exp_a = np.exp(a)\n",
        "    sum_exp_a = np.sum(exp_a)\n",
        "    y = exp_a / sum_exp_a\n",
        "    \n",
        "    return y\n",
        "\n",
        "y_predict = []\n",
        "for words in hidden:\n",
        "    tmp = []\n",
        "    for word in words:\n",
        "        tmp.append(softmax(np.dot(h2y.T, word)))\n",
        "    y_predict.append(tmp)\n",
        "\n",
        "hidden, y_predict"
      ],
      "execution_count": 176,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([[array([0.06754982, 0.0545118 , 0.22466635, 0.54402886])],\n",
              "  [array([0.37476888, 0.39952094, 0.25316527, 0.13961748]),\n",
              "   array([0.97403416, 0.71751094, 0.87964693, 0.28081678])],\n",
              "  [array([0.06754982, 0.0545118 , 0.22466635, 0.54402886]),\n",
              "   array([0.19125611, 0.1286674 , 0.50945568, 0.81840018])],\n",
              "  [array([0.97403416, 0.71751094, 0.87964693, 0.28081678]),\n",
              "   array([0.85877328, 0.32173556, 0.64433476, 0.89671025])],\n",
              "  [array([0.19125611, 0.1286674 , 0.50945568, 0.81840018]),\n",
              "   array([0.37476888, 0.39952094, 0.25316527, 0.13961748])],\n",
              "  [array([0.85877328, 0.32173556, 0.64433476, 0.89671025]),\n",
              "   array([0.25662537, 0.19846585, 0.20178147, 0.47612152])],\n",
              "  [array([0.37476888, 0.39952094, 0.25316527, 0.13961748])]],\n",
              " [[array([0.16948908, 0.18200717, 0.16688422, 0.18484098, 0.14650353,\n",
              "          0.15027501])],\n",
              "  [array([0.18157823, 0.20034417, 0.13626636, 0.22973571, 0.14589326,\n",
              "          0.10618226]),\n",
              "   array([0.16946556, 0.23748662, 0.09621911, 0.34458805, 0.09551736,\n",
              "          0.0567233 ])],\n",
              "  [array([0.16948908, 0.18200717, 0.16688422, 0.18484098, 0.14650353,\n",
              "          0.15027501]),\n",
              "   array([0.16755018, 0.19816678, 0.15859847, 0.21611568, 0.12685746,\n",
              "          0.13271142])],\n",
              "  [array([0.16946556, 0.23748662, 0.09621911, 0.34458805, 0.09551736,\n",
              "          0.0567233 ]),\n",
              "   array([0.18390246, 0.2388948 , 0.12812982, 0.2742773 , 0.10220044,\n",
              "          0.07259518])],\n",
              "  [array([0.16755018, 0.19816678, 0.15859847, 0.21611568, 0.12685746,\n",
              "          0.13271142]),\n",
              "   array([0.18157823, 0.20034417, 0.13626636, 0.22973571, 0.14589326,\n",
              "          0.10618226])],\n",
              "  [array([0.18390246, 0.2388948 , 0.12812982, 0.2742773 , 0.10220044,\n",
              "          0.07259518]),\n",
              "   array([0.18013816, 0.19437778, 0.15451353, 0.20226271, 0.14549216,\n",
              "          0.12321568])],\n",
              "  [array([0.18157823, 0.20034417, 0.13626636, 0.22973571, 0.14589326,\n",
              "          0.10618226])]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 176
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rOaVTBYkq2Rx",
        "colab_type": "text"
      },
      "source": [
        "## **4. Loss 계산**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aud9KmXpiIBz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c848f0a3-d1de-41f0-b3e3-bd5df3fffee9"
      },
      "source": [
        "loss = []\n",
        "for predict in y_predict:\n",
        "    tmp = []\n",
        "    for j in range(len(predict)):\n",
        "        tmp.append(np.log(predict[j]) * y[j])\n",
        "    loss.append(np.sum(tmp))\n",
        "loss = -np.sum(loss)\n",
        "loss"
      ],
      "execution_count": 174,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "22.02069231548569"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 174
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yk_q2SZQq5JJ",
        "colab_type": "text"
      },
      "source": [
        "## **5. Back Propagation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AhBQIZVflD73",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        },
        "outputId": "786236a2-e5c4-4902-bed4-ab2e6fa7f6e7"
      },
      "source": [
        "alpha = 0.01\n",
        "diff = []\n",
        "h_layer = []\n",
        "x_layer = []\n",
        "\n",
        "for i in range(len(y)):\n",
        "    for j in range(len(y_predict[i])):\n",
        "        diff.append(y_predict[i][j] - y[i])\n",
        "        h_layer.append(hidden[i][j])\n",
        "        x_layer.append(x[i][j])\n",
        "\n",
        "h2y = h2y - alpha*np.dot(np.matrix(h_layer).T, np.matrix(diff))\n",
        "x2h = x2h - np.dot(np.matrix(x_layer).T, np.dot(h2y, np.matrix(diff).T).T)\n",
        "\n",
        "h2y, x2h"
      ],
      "execution_count": 205,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(matrix([[0.94253575, 0.95607237, 0.54788113, 0.91982228, 0.52564631,\n",
              "          0.03417574],\n",
              "         [0.72913612, 0.65420656, 0.23575613, 0.86925077, 0.71368975,\n",
              "          0.07544881],\n",
              "         [0.24869692, 0.52461004, 0.33515029, 0.84645276, 0.12458523,\n",
              "          0.59374935],\n",
              "         [0.41530776, 0.40663807, 0.39129635, 0.25839081, 0.23353449,\n",
              "          0.27442211]]),\n",
              " matrix([[ 1.14314743,  0.87548047,  1.35643243,  0.814836  ],\n",
              "         [ 0.65323489,  0.89434217, -0.53389528,  0.25139187],\n",
              "         [-1.09725366, -0.82141313,  0.46088537,  0.34299834],\n",
              "         [-0.33057984, -0.62806556,  0.30645926,  0.90661075],\n",
              "         [-0.65989954, -0.71699256,  0.62958885,  0.56033126],\n",
              "         [-0.01053061,  0.38955723,  0.04006817, -0.0777082 ]]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 205
        }
      ]
    }
  ]
}